## Pyspark Data Transformations
In the presented code, I executed a range of transformations on Spark dataframes. These operations encompassed tasks like converting RDD to dataframe, utilizing filters and maps for data manipulation, executing unions, and reading data from diverse file formats like XML, CSV, JSON, and Parquet. I incorporated split operations and employed case statements within my data processing pipeline to enhance functionality. Furthermore, I established unified column lists, ensuring consistency across dataframes. I delved into RDD column-level processing and crafted a schema using StructType and NamedTuple to structure and organize the data effectively.

## Installation 
Begin by cloning the repository to your local machine. Execute the following command in your terminal or command prompt:
sh
Copy code
git clone https://github.com/DE11priyatam/Transformations.git
Ensure that PySpark is installed on your local machine.
Import the project into your preferred Integrated Development Environment (IDE).
Ensure you've configured the correct file paths necessary for the code to execute without errors.
Build and run the project to observe its functionality

## Technologies/Tools Used
* PySpark: PySpark is a versatile and high-performance distributed processing framework. To install PySpark, execute the following command in your terminal:
```sh
pip install pyspark==<PySparkVersion>
```
* Python: Python is a powerful high-level programming language. You can download the latest version of Python from the official website: <https://www.python.org/downloads/>
* PyCharm: PyCharm serves as an integrated development environment (IDE) for Python programming. Download PyCharm from the official website: <https://www.jetbrains.com/pycharm/download/#section=windows>


## Documentation
PySpark Documentation - <https://spark.apache.org/docs/latest/>
